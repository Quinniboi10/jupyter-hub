{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import isnan\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "train_seq_df = pd.read_csv(\"./datasets/stanford-rna-3d-folding-2/train_sequences.csv\")\n",
    "train_label_df = pd.read_csv(\"./datasets/stanford-rna-3d-folding-2/train_labels.csv\")\n",
    "\n",
    "train_label_df[\"chain\"] = train_label_df[\"chain\"].astype(str)\n",
    "train_label_df['target_id'] = train_label_df['ID'].str.split('_', n=1).str[0]\n",
    "\n",
    "nan_mask = train_label_df[['x_1','y_1','z_1']].isna().any(axis=1)\n",
    "train_label_df['valid'] = ~nan_mask\n",
    "\n",
    "train_label_df = train_label_df.sort_values([\"target_id\", \"chain\", \"copy\", \"resid\"]).reset_index(drop=True)\n",
    "\n",
    "train_label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seq_df = pd.read_csv(\"./datasets/stanford-rna-3d-folding-2/validation_sequences.csv\")\n",
    "valid_label_df = pd.read_csv(\"./datasets/stanford-rna-3d-folding-2/validation_labels.csv\")\n",
    "\n",
    "valid_label_df[\"chain\"] = valid_label_df[\"chain\"].astype(str)\n",
    "valid_label_df['target_id'] = valid_label_df['ID'].str.split('_', n=1).str[0]\n",
    "\n",
    "nan_mask = valid_label_df[['x_1','y_1','z_1']].isna().any(axis=1) | (valid_label_df[['x_1','y_1','z_1']] <= -1e17).any(axis=1)\n",
    "valid_label_df['valid'] = ~nan_mask\n",
    "\n",
    "valid_label_df = valid_label_df.sort_values([\"target_id\", \"chain\", \"copy\", \"resid\"]).reset_index(drop=True)\n",
    "\n",
    "valid_label_df = valid_label_df.copy()\n",
    "valid_label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map a 3 letter sequence to\n",
    "# a value for embedding\n",
    "def get_token_map():\n",
    "    token_map = {}\n",
    "    tokens = \"ACGU \"\n",
    "    i = 1\n",
    "    for c1 in tokens:\n",
    "        for c2 in tokens:\n",
    "            if c2 == ' ':\n",
    "                continue\n",
    "            for c3 in tokens:\n",
    "                token_map[c1 + c2 + c3] = i\n",
    "                i += 1\n",
    "    return token_map\n",
    "\n",
    "token_map = get_token_map()\n",
    "\n",
    "print(token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def build_delta_map(df):\n",
    "    id_map = {}\n",
    "    total = 0\n",
    "    min_found = 0\n",
    "    max_found = 0\n",
    "    # Builds a mask where NaNs are [None, None, None]\n",
    "    for row in tqdm(df.itertuples(index=False), desc = \"Build id->coordinate map\", total=len(df)):\n",
    "        key = (row.target_id, row.chain, row.copy)\n",
    "        if not row.valid:\n",
    "            id_map.setdefault(key, []).append([None,None,None])\n",
    "        else:\n",
    "            id_map.setdefault(key, []).append([row.x_1,row.y_1,row.z_1])\n",
    "\n",
    "    # Calculates relative distances and handles NaNs\n",
    "    # for shape in tqdm(id_map.values(), desc=\"Calculate deltas\"):\n",
    "    #     abs_coords = [p.copy() if p[0] is not None else None for p in shape]\n",
    "    \n",
    "    #     prev_abs = None\n",
    "    \n",
    "    #     for i in range(len(shape)):\n",
    "    #         if abs_coords[i] is None:\n",
    "    #             shape[i] = [None, None, None]\n",
    "    #             prev_abs = None\n",
    "    #             continue\n",
    "    \n",
    "    #         if prev_abs is None:\n",
    "    #             shape[i] = [None, None, None]\n",
    "    #         else:\n",
    "    #             shape[i] = [abs_coords[i][j] - prev_abs[j] for j in range(3)]\n",
    "    \n",
    "    #         prev_abs = abs_coords[i]\n",
    "\n",
    "    return id_map\n",
    "\n",
    "def get_stds(id_map):\n",
    "    sx = sy = sz = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for shape in id_map.values():\n",
    "        for x,y,z in shape:\n",
    "            if x is None: continue\n",
    "            sx += x*x\n",
    "            sy += y*y\n",
    "            sz += z*z\n",
    "            n += 1\n",
    "\n",
    "    std_x = math.sqrt(sx / n) or 1.0\n",
    "    std_y = math.sqrt(sy / n) or 1.0\n",
    "    std_z = math.sqrt(sz / n) or 1.0\n",
    "\n",
    "    print(f\"std : ({std_x:.4f}, {std_y:.4f}, {std_z:.4f})\")\n",
    "    return std_x, std_y, std_z\n",
    "\n",
    "def normalize(id_map, std_x, std_y, std_z):\n",
    "    total = 0\n",
    "    min_found = float(\"inf\")\n",
    "    max_found = float(\"-inf\")\n",
    "    n = 0\n",
    "\n",
    "    for shape in tqdm(id_map.values(), desc=\"Normalize deltas\"):\n",
    "        for p in shape:\n",
    "            if p[0] is None:\n",
    "                continue\n",
    "\n",
    "            p[0] /= std_x\n",
    "            p[1] /= std_y\n",
    "            p[2] /= std_z\n",
    "\n",
    "            total += math.sqrt(p[0]**2 + p[1]**2 + p[2]**2)\n",
    "            min_found = min(min_found, p[0], p[1], p[2])\n",
    "            max_found = max(max_found, p[0], p[1], p[2])\n",
    "            n += 1\n",
    "\n",
    "    print(\"Average subcomponent magnitude:\", total / n)\n",
    "    print(\"Min:\", min_found)\n",
    "    print(\"Max:\", max_found)\n",
    "\n",
    "    return id_map\n",
    "\n",
    "train_map = build_delta_map(train_label_df)\n",
    "valid_map = build_delta_map(valid_label_df)\n",
    "\n",
    "stds = get_stds(train_map)\n",
    "\n",
    "train_map = normalize(train_map, *stds)\n",
    "valid_map = normalize(valid_map, *stds)\n",
    "\n",
    "# Map (target_id, chain, copy) to a list of\n",
    "# coordinates representing the shape produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_df(id_map):\n",
    "    examples = []\n",
    "    for (tid, chain, copy), coords in tqdm(id_map.items()):\n",
    "        examples.append({\n",
    "            \"target_id\": tid,\n",
    "            \"chain\": chain,\n",
    "            \"copy\": copy,\n",
    "            \"length\": len(coords)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(examples)\n",
    "\n",
    "train_examples_df = get_example_df(train_map)\n",
    "valid_examples_df = get_example_df(valid_map)\n",
    "\n",
    "# Drop everything that Kabsch simply can't handle\n",
    "train_examples_df = train_examples_df.drop(train_examples_df[train_examples_df['length'] < 3].index)\n",
    "valid_examples_df = valid_examples_df.drop(valid_examples_df[valid_examples_df['length'] < 3].index)\n",
    "\n",
    "train_examples_df.reset_index(drop=True)\n",
    "valid_examples_df.reset_index(drop=True)\n",
    "\n",
    "train_examples_df.head()\n",
    "valid_examples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./datasets/stanford-rna-3d-folding-2/extra\")\n",
    "\n",
    "from parse_fasta_py import parse_fasta\n",
    "\n",
    "# Builds a map of\n",
    "# (ID, chain) -> sequence\n",
    "def build_seq_map(df):\n",
    "    seq_map = {}\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        chains = parse_fasta(row[\"all_sequences\"])\n",
    "        for primary_chain, (seq, chain_list) in chains.items():\n",
    "            for ch in chain_list:\n",
    "                seq_map[(row[\"target_id\"], ch)] = seq\n",
    "    return seq_map\n",
    "\n",
    "train_seq_map = build_seq_map(train_seq_df)\n",
    "valid_seq_map = build_seq_map(valid_seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 0\n",
    "for seq in train_seq_map.values():\n",
    "    max_seq_len = max(max_seq_len, len(seq))\n",
    "max_seq_len += 2 # Because of padding each side\n",
    "print(\"Longest sequence:\", max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ember import Learner\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from collections import deque\n",
    "from time import time\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cuda.enable_flash_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "test_bs = 1\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_str(k, s):\n",
    "    return [s[i:i+k] for i in range(len(s)-k+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_by_id(id: str) -> str:\n",
    "    if '_' in id:\n",
    "        id = id.split('_')[0]\n",
    "    for i, row in train_seq_df.iterrows():\n",
    "        if row['target_id'] == id:\n",
    "            return row['sequence']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(seq: str):\n",
    "    str_tokens = split_str(3, seq)\n",
    "    tokens = [token_map[key] for key in str_tokens]\n",
    "    return torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorWrapper():\n",
    "    def __init__(self, *args):\n",
    "        self.x = args\n",
    "\n",
    "    def to(self, device, **kargs):\n",
    "        self.x = tuple(x.to(device, **kargs) for x in self.x)\n",
    "        return self\n",
    "\n",
    "    def get(self):\n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, example_df, id_map, seq_map):\n",
    "        self.examples = example_df\n",
    "        self.id_map = id_map\n",
    "        self.seq_map = seq_map\n",
    "\n",
    "        self.examples = self.examples[self.examples.apply(self.has_any_valid, axis=1)].reset_index(drop=True)\n",
    "\n",
    "        # Load this stuff into numpy so it's just an array index\n",
    "        # instead of a dataframe index which is way slower\n",
    "        self.target_ids = self.examples[\"target_id\"].to_numpy()\n",
    "        self.chains     = self.examples[\"chain\"].to_numpy()\n",
    "        self.copies     = self.examples[\"copy\"].to_numpy()\n",
    "\n",
    "        print(f\"Found {len(self):,} data points\")\n",
    "\n",
    "    def has_any_valid(self, row):\n",
    "        key = (row[\"target_id\"], row[\"chain\"], row[\"copy\"])\n",
    "    \n",
    "        coords = self.id_map.get(key)\n",
    "        if coords is None:\n",
    "            return False\n",
    "    \n",
    "        return sum(c[0] is not None for c in coords) >= 3\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row from the examples dataframe\n",
    "        target_id = self.target_ids[idx]\n",
    "        chain     = self.chains[idx]\n",
    "        copy      = self.copies[idx]\n",
    "\n",
    "        # Construct the key and get a list of coordinates\n",
    "        key = (target_id, chain, copy)\n",
    "        coords = self.id_map[key]\n",
    "\n",
    "        # Get the sequence of the coordinates\n",
    "        # Pad each side so that the output head N is for the central\n",
    "        # nucleotide in 3mer N\n",
    "        seq = ' ' + self.seq_map[(target_id, chain)] + ' '\n",
    "        raw_seq = encode(seq)\n",
    "\n",
    "        clean = []\n",
    "        valid_flags = []\n",
    "\n",
    "        # Sanitize data to remove NaNs\n",
    "        # but making sure to mask them out\n",
    "        for c in coords:\n",
    "            if c[0] is None:\n",
    "                clean.append([0.0, 0.0, 0.0])\n",
    "                valid_flags.append(False)\n",
    "            else:\n",
    "                clean.append(c)\n",
    "                valid_flags.append(True)\n",
    "\n",
    "        L = len(clean)\n",
    "\n",
    "        target = torch.tensor(clean, dtype=torch.float32)\n",
    "        is_valid = torch.tensor(valid_flags, dtype=torch.bool)\n",
    "\n",
    "        seq_tensor = torch.zeros(max_seq_len, dtype=torch.long)\n",
    "        target_tensor = torch.zeros(max_seq_len, 3, dtype=torch.float32)\n",
    "        valid_tensor = torch.zeros(max_seq_len, dtype=torch.bool)\n",
    "\n",
    "        seq_tensor[:L] = raw_seq[:]\n",
    "        target_tensor[:L, :] = target[:, :]\n",
    "        valid_tensor[:L] = is_valid[:]\n",
    "\n",
    "        # Should return dimensions\n",
    "        # [len]\n",
    "        # [len]\n",
    "        # [len, 3]\n",
    "        # and then collate_fn will pad to the batch\n",
    "        assert seq_tensor.shape[0] == target_tensor.shape[0], f\"Sequence tensor has length of {seq_tensor.shape[0]} but target is {target_tensor.shape[0]}\"\n",
    "\n",
    "        return seq_tensor, valid_tensor, target_tensor\n",
    "\n",
    "    # Take a batch and stack it\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        seqs, is_valid, targets = zip(*batch)\n",
    "\n",
    "        B = len(batch)\n",
    "\n",
    "        seq_tensor = torch.zeros(B, max_seq_len, dtype=torch.long)\n",
    "        valid_tensor = torch.zeros(B, max_seq_len, dtype=torch.bool)\n",
    "        target_tensor = torch.zeros(B, max_seq_len, 3, dtype=torch.float32)\n",
    "\n",
    "        for i, (s, v, t) in enumerate(zip(seqs, is_valid, targets)):\n",
    "            seq_tensor[i] = s\n",
    "            valid_tensor[i] = v\n",
    "            target_tensor[i] = t\n",
    "    \n",
    "        return TensorWrapper(seq_tensor, valid_tensor), target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    RNADataset(train_examples_df, train_map, train_seq_map),\n",
    "    batch_size=bs,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    collate_fn=RNADataset.collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    RNADataset(valid_examples_df, valid_map, valid_seq_map),\n",
    "    batch_size=test_bs,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=RNADataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:00:35.523631Z",
     "iopub.status.busy": "2026-02-02T21:00:35.522531Z",
     "iopub.status.idle": "2026-02-02T21:00:35.542140Z",
     "shell.execute_reply": "2026-02-02T21:00:35.540436Z",
     "shell.execute_reply.started": "2026-02-02T21:00:35.523559Z"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(dim, dim, 5, stride=1, padding='same')\n",
    "        self.conv2 = nn.Conv1d(dim, dim, 5, stride=1, padding='same', dilation=3)\n",
    "        self.conv3 = nn.Conv1d(dim, dim, 5, stride=1, padding='same', dilation=6)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, D, L)\n",
    "        # Run all conv layers as residuals\n",
    "        res = x\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.act(self.conv3(x))\n",
    "        return x + res\n",
    "\n",
    "class GlobalBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout, n_heads):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=256,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                norm_first=True,\n",
    "                bias=True\n",
    "            ),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: (B, L, D)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dim=128, n_heads=4, dropout=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input block\n",
    "        self.embed = nn.Embedding(len(token_map) + 1, dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len + 1, dim, padding_idx=0)\n",
    "\n",
    "        # Relationship block\n",
    "        self.local = ConvBlock(dim)\n",
    "        self.dist = GlobalBlock(dim, dropout, n_heads)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq = seq.get()[0]\n",
    "        B, L = seq.shape\n",
    "        # seq: (B, L)\n",
    "\n",
    "        # Input block\n",
    "        mask = seq == 0 # True means padding\n",
    "        x = self.embed(seq) # (B, L, D)\n",
    "\n",
    "        pos = torch.arange(1, max_seq_len + 1, device=seq.device).unsqueeze(0).masked_fill(mask, 0) # (B, L)\n",
    "        x = x + self.pos_emb(pos) # (B, L, D)\n",
    "\n",
    "        # Relationship block\n",
    "        for _ in range(5):\n",
    "            res = x\n",
    "            x = x.transpose(-1, -2) # (B, D, L)\n",
    "            x = self.local(x) # (B, D, L)\n",
    "            x = x.transpose(-1, -2) # (B, L, D)\n",
    "\n",
    "            x = self.dist(x, mask) # Ideally this is pairwise but VRAM is hard to come by\n",
    "\n",
    "            x = self.fc1(x)\n",
    "\n",
    "            x = x + res\n",
    "\n",
    "        # Decoding block\n",
    "        x = self.head(x) # (B, L, 3)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch(out, tgt, mask):\n",
    "    dtype = out.dtype\n",
    "    B, L, _ = out.shape\n",
    "    m = mask.unsqueeze(-1) # (B,L,1)\n",
    "\n",
    "    out = out.to(torch.float32)\n",
    "    tgt = tgt.to(torch.float32)\n",
    "    m = m.to(torch.float32)\n",
    "\n",
    "    # First calculate the translation vector\n",
    "    # that would be applied so out and tgt\n",
    "    # are both centered\n",
    "    # Making sure to multiply by m so the\n",
    "    # bad coordinates remain masked out\n",
    "\n",
    "    # Calculates (B, 1, 1) / (B, 1, 1)\n",
    "    mu_out = (out * m).sum(dim=1, keepdim=True) / (m.sum(dim=1, keepdim=True) + 1e-8)\n",
    "    mu_tgt = (tgt * m).sum(dim=1, keepdim=True) / (m.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    out = out - mu_out\n",
    "    tgt = tgt - mu_tgt\n",
    "\n",
    "    # Get the cross-variance matrix\n",
    "    cross_var = (out * m).transpose(-1, -2) @ (tgt * m)\n",
    "    # Then find the singular value decomposition\n",
    "    U, _, Vt = torch.linalg.svd(cross_var)\n",
    "\n",
    "    rot_mat = Vt.transpose(-1,-2) @ U.transpose(-1,-2)\n",
    "    flip = torch.det(rot_mat) < 0.0\n",
    "    if flip.any().item():\n",
    "        Vt = Vt.clone()\n",
    "        Vt[flip, -1, :] *= -1.0\n",
    "        rot_mat = Vt.transpose(-1,-2) @ U.transpose(-1,-2)\n",
    "    return mu_out.to(dtype), mu_tgt.to(dtype), rot_mat.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "    def forward(self, out, tgt):\n",
    "        x = self.learner.x\n",
    "        assert isinstance(x, TensorWrapper)\n",
    "        assert torch.isfinite(out).all(), \"out has NaNs/Infs, likely from gradient explosions\"\n",
    "        assert torch.isfinite(tgt).all(), \"tgt has NaNs/Infs, likely from input data\"\n",
    "        _, mask = x.get()\n",
    "        m = mask.unsqueeze(-1)\n",
    "        return (((out - tgt)**2) * m).sum() / (m.sum() + 1e-8)\n",
    "\n",
    "# Based on https://hunterheidenreich.com/posts/kabsch-algorithm/\n",
    "# Almost identical to the article but with masking\n",
    "\n",
    "# Also it must do all computations in f32\n",
    "# to avoid weird NaNs everywhere\n",
    "class KabschRMSDLoss(nn.Module):\n",
    "    def forward(self, out, tgt):\n",
    "        if self.learner.model.training:\n",
    "            tgt = tgt + torch.rand_like(tgt, device=tgt.device) * 0.01\n",
    "        with torch.amp.autocast(device_type=self.learner.device, enabled=False):\n",
    "            B, L, _ = out.shape\n",
    "            _, mask = self.learner.x.get()\n",
    "            m = mask.unsqueeze(-1) # (B,L,1)\n",
    "\n",
    "            out = out.to(torch.float32)\n",
    "            tgt = tgt.to(torch.float32)\n",
    "            m = m.to(torch.float32)\n",
    "\n",
    "            # First calculate the translation vector\n",
    "            # that would be applied, out and tgt\n",
    "            # are both centered\n",
    "            # Making sure to multiply by w so the\n",
    "            # bad coordinates remain masked out\n",
    "\n",
    "            # Calculates (B, 1, 1) / (B, 1, 1)\n",
    "            mu_out = (out * m).sum(dim=1, keepdim=True) / (m.sum(dim=1, keepdim=True) + 1e-8)\n",
    "            mu_tgt = (tgt * m).sum(dim=1, keepdim=True) / (m.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "            out = out - mu_out\n",
    "            tgt = tgt - mu_tgt\n",
    "\n",
    "            # Get the cross-variance matrix\n",
    "            cross_var = (out * m).transpose(-1, -2) @ (tgt * m)\n",
    "            # Then find the singular value decomposition\n",
    "            U, _, Vt = torch.linalg.svd(cross_var)\n",
    "\n",
    "            rot_mat = Vt.transpose(-1,-2) @ U.transpose(-1,-2)\n",
    "            flip = torch.det(rot_mat) < 0.0\n",
    "            if flip.any().item():\n",
    "                Vt = Vt.clone() # Clone so gradients don't get destroyed\n",
    "                Vt[flip, -1, :] *= -1.0\n",
    "                rot_mat = Vt.transpose(-1,-2) @ U.transpose(-1,-2)\n",
    "\n",
    "            out_aligned = out @ rot_mat\n",
    "            diff = (tgt - out_aligned) * m\n",
    "\n",
    "            # Making sure to average each sample\n",
    "            rmsd = torch.sqrt((diff ** 2).sum(dim=(-1, -2)) / (m.sum(dim=(-1, -2)) + 1e-8) + 1e-8) # (B)\n",
    "\n",
    "            if not torch.isfinite(rmsd).all():\n",
    "                return torch.zeros((), device=out.device, requires_grad=True)\n",
    "\n",
    "            return rmsd.mean()\n",
    "\n",
    "class PairwiseDistanceLoss(nn.Module):\n",
    "    # If K is None, then just to the entire thing\n",
    "    def __init__(self, K=512):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, out, tgt):\n",
    "        if self.training:\n",
    "            tgt = tgt + torch.rand_like(tgt) * 0.01\n",
    "\n",
    "        _, mask = self.learner.x.get() # (B, L)\n",
    "        B, L = mask.shape\n",
    "\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        tgt = tgt * mask.unsqueeze(-1)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        mask_sum = 0.0\n",
    "\n",
    "        if self.K is None:\n",
    "            K = L\n",
    "        else:\n",
    "            K = self.K\n",
    "        for k in range(1, K + 1):\n",
    "            out_i = out[:, :-k] # (B, L-k, D)\n",
    "            out_j = out[:, k:] # (B, L-k, D)\n",
    "\n",
    "            tgt_i = tgt[:, :-k]\n",
    "            tgt_j = tgt[:, k:]\n",
    "\n",
    "            mask_i = mask[:, :-k]\n",
    "            mask_j = mask[:, k:]\n",
    "\n",
    "            pair_mask = mask_i * mask_j # (B, L-k)\n",
    "\n",
    "            # Get distances (diff then get magnitude)\n",
    "            out_dist = (out_i - out_j).norm(dim=-1)\n",
    "            tgt_dist = (tgt_i - tgt_j).norm(dim=-1)\n",
    "\n",
    "            diff = (tgt_dist - out_dist).pow(2)\n",
    "\n",
    "            loss_sum += (diff * pair_mask).sum()\n",
    "            mask_sum += pair_mask.sum()\n",
    "\n",
    "        return loss_sum / (mask_sum + 1e-8)\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha = 0.1, beta = 1.0):\n",
    "        super().__init__()\n",
    "        self.a = alpha\n",
    "        self.b = beta\n",
    "        self.loss_a = PairwiseDistanceLoss()\n",
    "        self.loss_b = KabschRMSDLoss()\n",
    "    def forward(self, out, tgt):\n",
    "        self.loss_a.learner = self.learner\n",
    "        self.loss_b.learner = self.learner\n",
    "        loss_a = self.loss_a(out, tgt)\n",
    "        loss_b = self.loss_b(out, tgt)\n",
    "        return loss_a * self.a + loss_b * self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ember.metric import *\n",
    "\n",
    "class AvgDistMetric(ExpAvgMetric):\n",
    "    is_train = True\n",
    "    do_reporting = True\n",
    "    name = \"avg dist\"\n",
    "\n",
    "    def step(self):\n",
    "        with torch.amp.autocast(device_type=self.learner.device, enabled=False), torch.no_grad():\n",
    "            out = self.learner.preds\n",
    "            tgt = self.learner.y\n",
    "            _, mask = self.learner.x.get()\n",
    "            out_t, tgt_t, rot_mat = kabsch(out, tgt, mask)\n",
    "\n",
    "            out = out - out_t\n",
    "            tgt = tgt - tgt_t\n",
    "\n",
    "            out_aligned = out @ rot_mat\n",
    "            diff = (tgt - out_aligned) * mask.unsqueeze(-1)\n",
    "\n",
    "            # Making sure to average each sample\n",
    "            rmsd = torch.sqrt((diff ** 2).sum(dim=(-1, -2)) / (mask.sum(dim=(-1, -2)) + 1e-8) + 1e-8) # (B)\n",
    "\n",
    "            self.avg.step(float(rmsd.mean().detach().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ember import Learner\n",
    "from ember.callback import *\n",
    "\n",
    "model = Model()\n",
    "optim = torch.optim.AdamW(model.parameters())\n",
    "learner = Learner(model, optim, train_loader, test_loader, loss_fn=HybridLoss(), clip_grads=1, accum=16,\n",
    "                 metrics = [LRMetric(), TrainLossMetric(), ValidLossMetric(), AvgDistMetric()],\n",
    "                 cbs     = [SaveBestCallback(\"./exports/stanford/autosave\"), StatusCallback()]\n",
    ")\n",
    "\n",
    "learner.fit_one_cycle(num_epochs, max_lr=1e-3)\n",
    "\n",
    "learner.save(\"./exports/stanford/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot(\"train loss\", \"valid loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot(\"avg dist\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "sourceId": 118765,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
